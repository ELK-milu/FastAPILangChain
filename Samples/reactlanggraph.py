from typing import Type, Optional

from langchain_core.callbacks import CallbackManagerForToolRun, AsyncCallbackManagerForToolRun
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool, BaseTool
from langgraph.prebuilt.chat_agent_executor import AgentState
import json
from langchain_core.messages import ToolMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END
from IPython.display import Image, display
from pydantic import BaseModel, Field

model = ChatOpenAI(
    model="deepseek-ai/DeepSeek-V3",
    base_url="https://api.siliconflow.cn/v1",
    api_key="sk-lmfljfvthonnvdwhtvgbzhvvbttzccciuqbgkplziczcogaa",
)

'''
@tool
def get_weather(location: str):
    """Call to get the weather from a specific location."""
    # This is a placeholder for the actual implementation
    # Don't let the LLM know this though ğŸ˜Š
    if any([city in location.lower() for city in ["sf", "san francisco"]]):
        return "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
    else:
        return f"I am not sure what the weather is in {location}"
'''
class CalculatorInput(BaseModel):
    location: str = Field(description="the location to get the weather for")


class get_weather(BaseTool):
    name:str = "get_weather"
    description:str = "useful for when you need to answer questions about math"
    args_schema: Type[BaseModel] = CalculatorInput

    def _run(
        self, location: str, run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        if any([city in location.lower() for city in ["sf", "san francisco"]]):
            return "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
        else:
            return f"I am not sure what the weather is in {location}"

    async def _arun(
        self, location: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("get_weather does not support async")


tools = [get_weather()]

model = model.bind_tools(tools)


tools_by_name = {tool.name: tool for tool in tools}


# å®šä¹‰å·¥å…·è°ƒç”¨èŠ‚ç‚¹
def tool_node(state: AgentState):
    outputs = []
    for tool_call in state["messages"][-1].tool_calls:
        tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
        outputs.append(
            ToolMessage(
                content=json.dumps(tool_result),
                name=tool_call["name"],
                tool_call_id=tool_call["id"],
            )
        )
    return {"messages": outputs}


# å®šä¹‰æ¨¡å‹è°ƒç”¨èŠ‚ç‚¹
def call_model(
    state: AgentState,
    config: RunnableConfig,
):
    # å®šä¹‰promptæç¤ºè¯
    system_prompt = SystemMessage(
        "You are a helpful AI assistant, please respond to the users query to the best of your ability!"
    )
    response = model.invoke([system_prompt] + state["messages"], config)
    # éœ€è¦è¿”å›ä¸€ä¸ªmessagesåˆ—è¡¨
    return {"messages": [response]}


# å®šä¹‰å¾ªç¯ç»“æŸæ¡ä»¶
def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    # å¦‚æœæ— å·¥å…·è°ƒç”¨åˆ™ç»“æŸ
    if not last_message.tool_calls:
        return "end"
    else:
        return "continue"



# å®šä¹‰ä¸€ä¸ªå›¾
workflow = StateGraph(AgentState)

# å®šä¹‰å¾ªç¯çš„ä¸¤èŠ‚ç‚¹
workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)

workflow.set_entry_point("agent")

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    # èµ·ç‚¹ï¼šagentèŠ‚ç‚¹
    "agent",
    # è°ƒç”¨agentåçš„hookå‡½æ•°
    should_continue,
    # æ ¹æ®hookå‡½æ•°è¿”å›çš„ç»“æœè¿›è¡ŒèŠ‚ç‚¹è°ƒç”¨æ˜ å°„
    # è‹¥hookè¿”å›continueåˆ™è°ƒç”¨toolsèŠ‚ç‚¹ï¼Œè‹¥ä¸ºendåˆ™è°ƒç”¨ENDèŠ‚ç‚¹
    # ENDèŠ‚ç‚¹æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„èŠ‚ç‚¹ï¼Œå°±æ˜¯workflowçš„ç»“æŸ
    {
        # If `tools`, then we call the tool node.
        "continue": "tools",
        # Otherwise we finish.
        "end": END,
    },
)

# ä¸ºtoolsæ·»åŠ å›åˆ°agentçš„å¾ªç¯
workflow.add_edge("tools", "agent")

# ç¼–è¯‘workflowä¸ºä¸€ä¸ªgraphå¯¹è±¡
graph = workflow.compile()


agent_responses = []
tool_responses = []


# å¯¹æ¶ˆæ¯å¯¹è±¡è¿›è¡Œæµå¼è¾“å‡º
def print_stream(stream):
    for chunk in stream:
        message_chunk, metadata = chunk
        node_name = metadata.get('langgraph_node', 'unknown')
        print(message_chunk)

        if hasattr(message_chunk, 'content') and message_chunk.content:
            if node_name == 'agent':
                agent_responses.append(message_chunk.content)
                #print(f"{message_chunk.content}", end="", flush=True)
            elif node_name == 'tools':
                tool_responses.append(message_chunk.content)
                #print(f"tools:{message_chunk.content}")

inputs = {"messages": [("user", "sfå¤©æ°”æ€ä¹ˆæ ·")]}
print_stream(graph.stream(inputs, stream_mode="messages"))