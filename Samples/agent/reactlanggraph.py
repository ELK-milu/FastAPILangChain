from typing import Type, Optional

from langchain_core.callbacks import CallbackManagerForToolRun, AsyncCallbackManagerForToolRun
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool, BaseTool
from langgraph.prebuilt.chat_agent_executor import AgentState
from langchain_core.messages import ToolMessage, SystemMessage
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field

from utils.ChatNode import create_chat_node
from utils.ConditionNode import should_continue
from utils.OutputParser import agent_with_tool_stream_parser
from utils.ToolNode import create_tool_node
from Samples import model

class CalculatorInput(BaseModel):
    location: str = Field(description="the location to get the weather for")


class get_weather(BaseTool):
    name:str = "get_weather"
    description:str = "useful for when you need to answer questions about math"
    args_schema: Type[BaseModel] = CalculatorInput

    def _run(
        self, location: str, run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool."""
        if any([city in location.lower() for city in ["sf", "san francisco"]]):
            return "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
        else:
            return f"I am not sure what the weather is in {location}"

    async def _arun(
        self, location: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None
    ) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("get_weather does not support async")


tools = [get_weather()]

model = model.bind_tools(tools)
# ä½¿ç”¨ç¤ºä¾‹
generic_tool_node = create_tool_node(tools)
system_prompt = SystemMessage(
    "You are a helpful AI assistant, please respond to the users query to the best of your ability!"
)
call_chat_node = create_chat_node(model,system_prompt)
# å®šä¹‰å¾ªç¯ç»“æŸæ¡ä»¶

# å®šä¹‰ä¸€ä¸ªå›¾
workflow = StateGraph(AgentState)

# å®šä¹‰å¾ªç¯çš„ä¸¤èŠ‚ç‚¹
workflow.add_node("agent", call_chat_node)
workflow.add_node("tools", generic_tool_node)

workflow.set_entry_point("agent")

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    # èµ·ç‚¹ï¼šagentèŠ‚ç‚¹
    "agent",
    # è°ƒç”¨agentåçš„hookå‡½æ•°
    should_continue,
    # æ ¹æ®hookå‡½æ•°è¿”å›çš„ç»“æœè¿›è¡ŒèŠ‚ç‚¹è°ƒç”¨æ˜ å°„
    # è‹¥hookè¿”å›continueåˆ™è°ƒç”¨toolsèŠ‚ç‚¹ï¼Œè‹¥ä¸ºendåˆ™è°ƒç”¨ENDèŠ‚ç‚¹
    # ENDèŠ‚ç‚¹æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„èŠ‚ç‚¹ï¼Œå°±æ˜¯workflowçš„ç»“æŸ
    {
        # If `tools`, then we call the tool node.
        "continue": "tools",
        # Otherwise we finish.
        "end": END,
    },
)

# ä¸ºtoolsæ·»åŠ å›åˆ°agentçš„å¾ªç¯
workflow.add_edge("tools", "agent")

# ç¼–è¯‘workflowä¸ºä¸€ä¸ªgraphå¯¹è±¡
graph = workflow.compile()


inputs = {"messages": [("user", "sfå¤©æ°”æ€ä¹ˆæ ·")]}
agent_with_tool_stream_parser(graph.stream(inputs, stream_mode="messages"))
